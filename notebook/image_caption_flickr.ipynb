{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "from torchtext.datasets import translation, imdb, language_modeling, nli\n",
    "from torchtext.datasets import sequence_tagging, unsupervised_learning, text_classification, sst\n",
    "from torchtext.data import  Field, RawField,  BucketIterator\n",
    "\n",
    "import spacy\n",
    "import statistics\n",
    "import torchtext\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.metrics import Accuracy\n",
    "\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://spacy.io/usage/spacy-101#annotations-token\n",
    "# spacy_de = spacy.load('de_core_news_sm')\n",
    "# spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "# def tokenize_de(text: str) -> List[str]:\n",
    "#     return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "# def tokenize_en(text: str) -> List[str]:\n",
    "#     return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# source = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "# target = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "\n",
    "\n",
    "# train_data, valid_data, test_data = translation.WMT14.splits(\n",
    "#     exts=('.de', '.en'), fields=(source, target), root='../data/translation/'\n",
    "# )\n",
    "# print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "# print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "# print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_eng = spacy.load('en_core_web_sm')\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]\n",
    "\n",
    "\n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root_dir, caption_file, transform=None, freq_threshold=5,\n",
    "                 train=True, split_val=0.2):\n",
    "        self.root_dir = root_dir\n",
    "        self.caption_file = caption_file\n",
    "        self.df = pd.read_csv(caption_file)\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.df['caption'].tolist())\n",
    "        \n",
    "        self.train = train \n",
    "        self.split_val = split_val\n",
    "        self._do_split_train_valid()\n",
    "        \n",
    "#         # Get img, caption columns\n",
    "#         self.imgs = self.df[\"image\"]\n",
    "#         self.captions = self.df[\"caption\"]\n",
    "\n",
    "        # Initialize vocabulary and build vocab\n",
    "\n",
    "        \n",
    "    def _do_split_train_valid(self):\n",
    "        imgs_train, imgs_valid, caps_train, caps_valid = train_test_split(\n",
    "            self.df[\"image\"], self.df[\"caption\"], \n",
    "            test_size=self.split_val, random_state=16\n",
    "        )\n",
    "        \n",
    "        if self.train:\n",
    "            self.imgs = imgs_train\n",
    "            self.captions = caps_train\n",
    "        else:\n",
    "            self.imgs = imgs_valid\n",
    "            self.captions = caps_valid\n",
    "            \n",
    "        self.imgs = self.imgs.tolist()\n",
    "        self.captions = self.captions.tolist()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def _numericalized_caption(self, caption):\n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "        \n",
    "        return numericalized_caption\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        caption = self.captions[index]\n",
    "        img_id = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        ncaption = self._numericalized_caption(caption)\n",
    "\n",
    "        return img, torch.tensor(ncaption)\n",
    "\n",
    "\n",
    "class CaptionCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx)\n",
    "\n",
    "        return imgs, targets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flickr8k_dataloader(root_folder, caption_file, transform, train=True,\n",
    "                        batch_size=32, num_workers=8, shuffle=True, pin_memory=True):\n",
    "    \n",
    "    dataset = FlickrDataset(root_folder, caption_file, transform=transform, train=train)\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=num_workers,\n",
    "                            shuffle=shuffle, pin_memory=pin_memory, \n",
    "                            collate_fn=CaptionCollate(pad_idx=pad_idx))\n",
    "    \n",
    "    return dataloader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "            transforms.Resize((356, 356)),\n",
    "            transforms.RandomCrop((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "root_dir = \"/data/flickr/flickr8k/images/\"\n",
    "caption_file = \"/data/flickr/flickr8k/captions.txt\"\n",
    "\n",
    "train_loader, trainset = flickr8k_dataloader(root_dir, caption_file, transform=train_transform, train=True)\n",
    "valid_loader, validset = flickr8k_dataloader(root_dir, caption_file, transform=valid_transform, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'the', 'clowns', 'are', 'striking', 'a', 'pose', 'for', 'the', 'camera', '.', '<EOS>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, cap = trainset[200]\n",
    "print([trainset.vocab.itos[token] for token in cap.tolist()])\n",
    "\n",
    "len(trainset), len(validset)\n",
    "type(trainset.vocab.stoi[\"<PAD>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'some', 'african', 'kids', 'playing', 'with', 'a', 'ball', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 25]), torch.Size([24]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, caps = next(iter(valid_loader))\n",
    "print([trainset.vocab.itos[token] for token in caps[0, :-1].tolist()])\n",
    "\n",
    "caps.shape, caps[0, :-1].shape\n",
    "\n",
    "# caps[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size, train_CNN=False):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.train_CNN = train_CNN\n",
    "        self.resnet = models.resnet34(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.times = []\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        return self.dropout(self.relu(features))\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, dropout_p=0.5):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, dropout=dropout_p, \n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_size * 2, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        captions = captions[:, :-1]\n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class ImageCaptionNet(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(ImageCaptionNet, self).__init__()\n",
    "        self.encoderCNN = EncoderCNN(embed_size)\n",
    "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoderCNN(images)\n",
    "        outputs = self.decoderRNN(features, captions)\n",
    "        return outputs\n",
    "\n",
    "    def caption_image(self, image, vocabulary, max_length=50):\n",
    "        result_caption = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.encoderCNN(image).unsqueeze(0)\n",
    "            states = None\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
    "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
    "                predicted = output.argmax(1)\n",
    "                result_caption.append(predicted.item())\n",
    "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
    "\n",
    "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
    "                    break\n",
    "\n",
    "        return [vocabulary.itos[idx] for idx in result_caption]\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionTask(pl.LightningModule):\n",
    "    def __init__(self, model, optimizers, criterion, scheduler=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.scheduler = scheduler\n",
    "        self.metric = Accuracy()\n",
    "        \n",
    "    def forward(self, imgs, captions):\n",
    "        outputs = self.model(imgs, captions)\n",
    "        return outputs\n",
    "        \n",
    "    \n",
    "    def shared_step(self, batch, batch_idx):\n",
    "        imgs, captions = batch\n",
    "        outputs = self.model(imgs, captions)\n",
    "        \n",
    "        outputs_preprocess = outputs.reshape(-1, outputs.shape[2])\n",
    "        captions_preprocess = captions.reshape(-1)\n",
    "        loss = criterion(outputs_preprocess, captions_preprocess)\n",
    "#         acc = (output.argmax(1) == cls).sum().item()\n",
    "        acc = self.metric(outputs_preprocess.argmax(1), captions_preprocess)\n",
    "        return loss, acc\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self.shared_step(batch, batch_idx)\n",
    "        result = pl.TrainResult(loss)\n",
    "#         result.log_dict({'trn_loss': loss})\n",
    "        result.log_dict({'trn_loss': loss, 'trn_acc':acc})\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc = self.shared_step(batch, batch_idx)\n",
    "        result = pl.EvalResult(checkpoint_on=loss)\n",
    "#         result.log_dict({'val_loss': loss})\n",
    "        result.log_dict({'val_loss': loss, 'val_acc': acc})\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if self.scheduler:\n",
    "            return [self.optimizer], [self.scheduler]\n",
    "        return self.optimizer\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import save_checkpoint, load_checkpoint, print_examples\n",
    "# from get_loader import get_loader\n",
    "# from model import CNNtoRNN\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "vocab_size = len(trainset.vocab)\n",
    "num_layers = 1\n",
    "learning_rate = 2e-4\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2994"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model, loss etc\n",
    "model = ImageCaptionNet(embed_size, hidden_size, vocab_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_index = trainset.vocab.stoi[\"<PAD>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '../saved_model'\n",
    "# DEFAULTS used by the Trainer\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor='checkpoint_on',\n",
    "    mode='min',\n",
    "    prefix='image_caption_net_'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | ImageCaptionNet  | 24 M  \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "2 | metric    | Accuracy         | 0     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82de52c963594f6096a3a19d7d05ba33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00000: val_checkpoint_on reached 1.61182 (best 1.61182), saving model to ../saved_model/image_caption_net_epoch=0.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_checkpoint_on reached 0.94836 (best 0.94836), saving model to ../saved_model/image_caption_net_epoch=1.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_checkpoint_on reached 0.63462 (best 0.63462), saving model to ../saved_model/image_caption_net_epoch=2.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_checkpoint_on reached 0.45017 (best 0.45017), saving model to ../saved_model/image_caption_net_epoch=3.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_checkpoint_on reached 0.33191 (best 0.33191), saving model to ../saved_model/image_caption_net_epoch=4.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: val_checkpoint_on reached 0.25035 (best 0.25035), saving model to ../saved_model/image_caption_net_epoch=5.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_checkpoint_on reached 0.19440 (best 0.19440), saving model to ../saved_model/image_caption_net_epoch=6.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_checkpoint_on reached 0.15382 (best 0.15382), saving model to ../saved_model/image_caption_net_epoch=7.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00008: val_checkpoint_on reached 0.12294 (best 0.12294), saving model to ../saved_model/image_caption_net_epoch=8.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: val_checkpoint_on reached 0.10149 (best 0.10149), saving model to ../saved_model/image_caption_net_epoch=9.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00010: val_checkpoint_on reached 0.08382 (best 0.08382), saving model to ../saved_model/image_caption_net_epoch=10.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00011: val_checkpoint_on reached 0.07000 (best 0.07000), saving model to ../saved_model/image_caption_net_epoch=11.ckpt as top 1\n"
     ]
    }
   ],
   "source": [
    "tb_logger = pl_loggers.TensorBoardLogger('logs/flickr8k')\n",
    "task = ImageCaptionTask(model, optimizer, criterion)\n",
    "trainer = pl.Trainer(gpus=1, logger=tb_logger, checkpoint_callback=checkpoint_callback)\n",
    "trainer.fit(task, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models.resnet34(pretrained=True)\n",
    "# models.resnet50(pretrained=True)\n",
    "# models.resnet101(pretrained=True)\n",
    "# models.resnet152(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
